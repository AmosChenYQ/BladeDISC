

<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Functionality &mdash; BladeDISC 1.0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../../../../_static/css/theme.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../../../_static/css/theme.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../../../_static/copybutton.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../search.html" /> 

</head>

<body>
    <header>
        <div class="container">
            <a class="site-nav-toggle hidden-lg-up"><i class="icon-menu"></i></a>
            <a class="site-title" href="../../../../../../../../index.html">
                BladeDISC
            </a>
        </div>
    </header>


<div class="breadcrumbs-outer hidden-xs-down">
    <div class="container">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="breadcrumbs">
    
      <li><a href="../../../../../../../../index.html">Docs</a></li>
        
      <li>Functionality</li>
    
    
      <li class="breadcrumbs-aside">
        
            
            <a href="../../../../../../../../_sources/tao_compiler/tensorflow/compiler/mlir/disc/cutlass/media/docs/functionality.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>
</div>
    </div>
</div>
    <div class="main-outer">
        <div class="container">
            <div class="row">
                <div class="col-12 col-lg-3 site-nav">
                    
<div role="search">
    <form class="search" action="../../../../../../../../search.html" method="get">
        <div class="icon-input">
            <input type="text" name="q" placeholder="Search" />
            <span class="icon-search"></span>
        </div>
        <input type="submit" value="Go" class="d-hidden" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
    </form>
</div>
                    <div class="site-nav-tree">
                        
                            
                            
                                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../README.html">BladeDISC Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#what-s-new">What’s New</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#api-quickview">API QuickView</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#setup-and-examples">Setup and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#publications">Publications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#tutorials-and-documents-for-developers">Tutorials and Documents for Developers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#presentations-and-talks">Presentations and Talks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#how-to-contribute">How to Contribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#building-status">Building Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#faq">FAQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../README.html#contact-us">Contact Us</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/install_with_docker.html">Install with Docker</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/install_with_docker.html#download-a-bladedisc-docker-image">Download a BladeDISC Docker Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/install_with_docker.html#start-a-docker-container">Start a Docker Container</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/build_from_source.html">Build from Source</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/build_from_source.html#prerequisite">Prerequisite</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/build_from_source.html#checkout-the-source">Checkout the Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/build_from_source.html#launch-a-development-docker-container">Launch a development Docker container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/build_from_source.html#building-bladedisc-for-tensorflow-users">Building BladeDISC for TensorFlow Users</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/build_from_source.html#building-bladedisc-for-pytorch-users">Building BladeDISC for PyTorch Users</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/quickstart.html#quickstart-for-tensorflow-users">Quickstart for TensorFlow Users</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/quickstart.html#quickstart-for-pytorch-users">Quickstart for PyTorch Users</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/contribution.html">How to Contribute</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/contribution.html#local-development-environment">Local Development Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/contribution.html#submit-a-pull-request-to-bladedisc">Submit a Pull Request to BladeDISC</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/tutorials/index.html">Tutorials on Example Use Cases</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/tutorials/tensorflow_inference_and_training.html">Use case of TensorFlow Inference and Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../docs/tutorials/torch_bert_inference.html">Use case of PyTorch Inference</a></li>
</ul>
</li>
</ul>

                            
                        
                    </div>
                </div>
                <div class="col-12 col-lg-9">
                    <div class="document">
                        
                            
  <p><img alt="ALT" src="media/images/gemm-hierarchy-with-epilogue-no-labels.png" /></p>
<p><a class="reference external" href="/README.md#documentation">README</a> &gt; <strong>Functionality</strong></p>
<section id="functionality">
<h1>Functionality<a class="headerlink" href="#functionality" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p>N - Column Major Matrix</p></li>
<li><p>T - Row Major matrix</p></li>
<li><p>{N,T} x {N,T} - All combinations, i.e. NN, NT, TN, TT</p></li>
<li><p><a class="reference external" href="/include/cutlass/layout/tensor.h#L63-206">NHWC</a> - 4 dimension tensor used for convolution</p></li>
<li><p><a class="reference external" href="/include/cutlass/layout/tensor.h#L290-395">NCxHWx</a> - Interleaved 4 dimension tensor used for convolution</p></li>
<li><p>f - float point</p></li>
<li><p>s - signed int</p></li>
<li><p>b - bit</p></li>
<li><p>cf - complex float</p></li>
<li><p>bf16 - bfloat16</p></li>
<li><p>tf32 - tfloat32</p></li>
<li><p>Simt - Use Simt CUDA Core MMA</p></li>
<li><p>TensorOp - Use Tensor Core MMA</p></li>
<li><p>SpTensorOp - Use Sparse Tensor Core MMA</p></li>
<li><p>WmmaTensorOp - Use WMMA abstraction to use Tensor Core MMA</p></li>
</ul>
<section id="device-level-gemm">
<h2>Device-level GEMM<a class="headerlink" href="#device-level-gemm" title="Permalink to this heading">¶</a></h2>
<p>The following table summarizes device-level GEMM kernels in CUTLASS, organized by opcode class, data type, and layout.
Hyperlinks to relevant unit tests demonstrate how specific template instances may be defined.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Opcode Class</strong></th>
<th><strong>Compute Capability</strong></th>
<th><strong>CUDA Toolkit</strong></th>
<th><strong>Data Type</strong></th>
<th><strong>Layouts</strong></th>
<th><strong>Unit Test</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Simt</strong></td>
<td>50,60,61,70,75</td>
<td>9.2+</td>
<td><code>f32 * f32 + f32 =&gt; f32</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/simt_sgemm_nt_sm50.cu">example</a></td>
</tr>
<tr>
<td><strong>Simt</strong></td>
<td>50,60,61,70,75</td>
<td>9.2+</td>
<td><code>f64 * f64 + f64 =&gt; f64</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/simt_dgemm_nt_sm50.cu">example</a></td>
</tr>
<tr>
<td><strong>Simt</strong></td>
<td>60,61,70,75</td>
<td>9.2+</td>
<td><code>f16 * f16 + f16 =&gt; f16</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/simt_hgemm_nt_sm50.cu">example</a></td>
</tr>
<tr>
<td><strong>Simt</strong></td>
<td>61,70,75</td>
<td>9.2+</td>
<td><code>s8 * s8 + s32 =&gt; {s32,s8}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/simt_igemm_nt_sm50.cu">example</a></td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>70</td>
<td>9.2+</td>
<td><code>f16 * f16 + f16 =&gt; f16</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu">example</a></td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>70</td>
<td>9.2+</td>
<td><code>f16 * f16 + f32 =&gt; {f16, f32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu">example</a></td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>75</td>
<td>10.0+</td>
<td><code>s8 * s8 + s32 =&gt; {s32, s8}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu">example</a></td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>75</td>
<td>10.0+</td>
<td><code>s4 * s4 + s32 =&gt; {s32, s4}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_s4t_s4n_s4t_wmma_tensor_op_s32_sm75.cu">example</a></td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>75</td>
<td>10.0+</td>
<td><code>b1 ^ b1 + s32 =&gt; {s32, b1}</code></td>
<td>{ T } x { N } =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_b1t_b1n_b1t_wmma_tensor_op_s32_sm75.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>70</td>
<td>10.1+</td>
<td><code>f16 * f16 + f16 =&gt; f16</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>70</td>
<td>10.1+</td>
<td><code>f16 * f16 + f32 =&gt; {f16, f32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f32_sm70.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>75</td>
<td>10.2+</td>
<td><code>f16 * f16 + f16 =&gt; f16</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>75</td>
<td>10.2+</td>
<td><code>f16 * f16 + f32 =&gt; {f16, f32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm75.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>75</td>
<td>10.2+</td>
<td><code>s8 * s8 + s32 =&gt; {s32, s8}</code></td>
<td>{ T } x { N } =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>75</td>
<td>10.2+</td>
<td><code>s4 * s4 + s32 =&gt; {s32, s4}</code></td>
<td>{ T } x { N } =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>75</td>
<td>10.2+</td>
<td><code>b1 ^ b1 + s32 =&gt; {s32, b1}</code></td>
<td>{ T } x { N } =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>f16 * f16 + f16 =&gt; f16</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>f16 * f16 + f32 =&gt; {f16, f32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>bf16 * bf16 + f32 =&gt; {bf16, f32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_bf16n_bf16t_bf16t_tensor_op_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>tf32 * tf32 + f32 =&gt; f32</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>s8 * s8 + s32 =&gt; {s32, s8}</code></td>
<td>{ T } x { N } =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>s4 * s4 + s32 =&gt; {s32, s4}</code></td>
<td>{ T } x { N } =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>b1 ^ b1 + s32 =&gt; {s32, b1}</code></td>
<td>{ T } x { N } =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>f64 * f64 + f64 =&gt; f64</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>cf32 * cf32 + cf32 =&gt; cf32</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>cf64 * cf64 + cf64 =&gt; cf64</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu">example</a>, <a href="/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu">Gaussian 3m</a></td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>80</td>
<td>11.1+</td>
<td><code>f16 * f16 + f32 =&gt; {f16, f32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>80</td>
<td>11.1+</td>
<td><code>bf16 * bf16 + f32 =&gt; {bf16, f32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>80</td>
<td>11.1+</td>
<td><code>tf32 * tf32 + f32 =&gt; f32</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>80</td>
<td>11.1+</td>
<td><code>s8 * s8 + s32 =&gt; {s8, s32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>80</td>
<td>11.1+</td>
<td><code>s4 * s4 + s32 =&gt; {s4, s32}</code></td>
<td>{N,T} x {N,T} =&gt; {N,T}</td>
<td><a href="/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu">example</a></td>
</tr>
</tbody>
</table></section>
<section id="device-level-implicit-gemm-convolution">
<h2>Device-level Implicit GEMM convolution<a class="headerlink" href="#device-level-implicit-gemm-convolution" title="Permalink to this heading">¶</a></h2>
<p>The following table summarizes device-level implicit GEMM convolution kernels in CUTLASS, organized by opcode class, data type, and layout.
Hyperlinks to relevant conv2d fprop unit tests demonstrate how specific template instances may be defined.
One can find and/or create equivalent dgrad and wgrad convolutional operators.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Opcode Class</strong></th>
<th><strong>Compute Capability</strong></th>
<th><strong>CUDA Toolkit</strong></th>
<th><strong>Data Type</strong></th>
<th><strong>Layouts</strong></th>
<th><strong>Unit Test</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Simt</strong></td>
<td>50,60,61,70,75</td>
<td>9.2+</td>
<td><code>f32 * f32 + f32 =&gt; f32</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu">example</a></td>
</tr>
<tr>
<td><strong>Simt</strong></td>
<td>50,60,61,70,75</td>
<td>9.2+</td>
<td><code>cf32 * cf32 + cf32 =&gt; cf32</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>70</td>
<td>10.1+</td>
<td><code>f16 * f16 + f32 =&gt; {f16, f32}</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>75</td>
<td>10.2+</td>
<td><code>f16 * f16 + f32 =&gt; {f16, f32}</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>75</td>
<td>10.2+</td>
<td><code>s8 * s8 + s32 =&gt; {s32, s8}</code></td>
<td>NHWC, NCxHWx</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu">example</a>, <a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu">ncxhwx</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>75</td>
<td>10.2+</td>
<td><code>s4 * s4 + s32 =&gt; {s32, s4}</code></td>
<td>NHWC, NCxHWx</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu">example</a>, <a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu">ncxhwx</a></td>
</tr>
<tr>
<td><strong>Simt</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>f32 * f32 + f32 =&gt; f32</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>Simt</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>cf32 * cf32 + cf32 =&gt; cf32</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>f16 * f16 + f32 =&gt; {f16, f32}</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>f16 * f16 + f16 =&gt; f16</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>tf32 * tf32 + f32 =&gt; f32</code></td>
<td>NHWC</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu">example</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>s8 * s8 + s32 =&gt; {s32, s8}</code></td>
<td>NHWC, NCxHWx</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu">example</a>, <a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu">ncxhwx</a></td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>80</td>
<td>11.0+</td>
<td><code>s4 * s4 + s32 =&gt; {s32, s4}</code></td>
<td>NHWC, NCxHWx</td>
<td><a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu">example</a>, <a href="/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu">ncxhwx</a></td>
</tr>
</tbody>
</table></section>
<section id="warp-level-matrix-multiply-with-tensor-cores">
<h2>Warp-level Matrix Multiply with Tensor Cores<a class="headerlink" href="#warp-level-matrix-multiply-with-tensor-cores" title="Permalink to this heading">¶</a></h2>
<p>The following table summarizes supported warp level shapes for each TensorOp instruction.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Opcode Class</strong></th>
<th><strong>Instruction Shape</strong></th>
<th><strong>Warp Shapes</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TensorOp</strong></td>
<td>8-by-8-by-4</td>
<td>32x32x4, 32x64x4, 64x32x4, 64x64x4</td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>16-by-8-by-8</td>
<td>32x32x8, 32x64x8, 64x32x8, 64x64x8</td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>16-by-8-by-16</td>
<td>32x32x16, 32x64x16, 64x32x16, 64x64x16</td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>8-by-8-by-16</td>
<td>32x32x16, 32x64x16, 64x32x16, 64x64x16</td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>8-by-8-by-32</td>
<td>32x32x32, 32x64x32, 64x32x32, 64x64x32</td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>16-by-8-by-32</td>
<td>32x32x32, 32x64x32, 64x32x32, 64x64x32</td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>16-by-8-by-64</td>
<td>32x32x64, 32x64x64, 64x32x64, 64x64x64</td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>8-by-8-by-128</td>
<td>32x32x128, 32x64x128, 64x32x128, 64x64x128</td>
</tr>
<tr>
<td><strong>TensorOp</strong></td>
<td>16-by-8-by-256</td>
<td>32x32x256, 32x64x256, 64x32x256, 64x64x256</td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>16-by-8-by-16</td>
<td>64x64x16, 64x32x16, 32x64x16, 32x32x16</td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>16-by-8-by-32</td>
<td>64x64x32, 64x32x32, 32x64x32, 32x32x32</td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>16-by-8-by-64</td>
<td>64x64x64, 64x32x64, 32x64x64, 32x32x64</td>
</tr>
<tr>
<td><strong>SpTensorOp</strong></td>
<td>16-by-8-by-128</td>
<td>64x64x128, 64x32x128, 32x64x128, 32x32x128</td>
</tr>
</tbody>
</table><p>TensorOp instructions depend on a permuted shared memory layout that can be efficiently
loaded from. The following tables summarize the destination shared memory layout that
can be targeted by matrix operands. It is assumed that each thread loads 128b vectors
from global memory with layout specified in the column “GMEM Layout.”</p>
<p><strong>TensorOp 8-by-8-by-4.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>half_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorVoltaTensorOpCongruous&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>A</strong></td>
<td><code>half_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorVoltaTensorOpCrosswise&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>half_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorVoltaTensorOpCrosswise&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>half_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorVoltaTensorOpCongruous&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>half_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>float</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 16-by-8-by-8.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>half_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>A</strong></td>
<td><code>half_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>half_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCrosswise&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>half_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCongruous&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>half_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>float</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 16-by-8-by-8.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>tfloat32_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;32&gt;</code></td>
</tr>
<tr>
<td><strong>A</strong></td>
<td><code>tfloat32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;32&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>tfloat32_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCrosswise&lt;32&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>tfloat32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCongruous&lt;32&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>float</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 16-by-8-by-16.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>half_t</code>, <code>bfloat16_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>A</strong></td>
<td><code>half_t</code>, <code>bfloat16_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>half_t</code>, <code>bfloat16_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCrosswise&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>half_t</code>, <code>bfloat16_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCongruous&lt;16&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>half_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>float</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 8-by-8-by-4.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>double</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;64&gt;</code></td>
</tr>
<tr>
<td><strong>A</strong></td>
<td><code>double</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;64&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>double</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCrosswise&lt;64&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>double</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCongruous&lt;64&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>double</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 8-by-8-by-16.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>int8_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;8&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>int8_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;8&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>int32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 16-by-8-by-32.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>int8_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;8&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>int8_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;8&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>int32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 8-by-8-by-32.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>int4b_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;4&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>int4b_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;4&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>int32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 16-by-8-by-64.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>int4b_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;4&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>int4b_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;4&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>int32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>TensorOp 8-by-8-by-128.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>bin1_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;4&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>bin1_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCongruous&lt;4&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>int32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>SpTensorOp 16-by-8-by-16.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>tfloat32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;32, 32&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>tfloat32_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCrosswise&lt;32, 32&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>float</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>SpTensorOp 16-by-8-by-32.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>half_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;16, 64&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>half_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCrosswise&lt;16, 64&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>float</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>SpTensorOp 16-by-8-by-64.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>int8_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;8, 128&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>int8_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCrosswise&lt;8, 128&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>int32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table><p><strong>SpTensorOp 16-by-8-by-128.</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>Element</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>int4b_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajorTensorOpCrosswise&lt;4, 256&gt;</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>int4b_t</code></td>
<td><code>ColumnMajor</code></td>
<td><code>ColumnMajorTensorOpCrosswise&lt;4, 256&gt;</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>int32_t</code></td>
<td><code>RowMajor</code></td>
<td><code>RowMajor</code></td>
</tr>
</tbody>
</table></section>
<section id="warp-level-matrix-multiply-with-cuda-wmma-api">
<h2>Warp-level Matrix Multiply with CUDA WMMA API<a class="headerlink" href="#warp-level-matrix-multiply-with-cuda-wmma-api" title="Permalink to this heading">¶</a></h2>
<p>The following table summarizes supported warp level shapes for each WmmaTensorOp instruction.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Opcode Class</strong></th>
<th><strong>Instruction Shape</strong></th>
<th><strong>Warp Shapes</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>16-by-16-by-16</td>
<td>32x32x16, 32x64x16, 64x32x16</td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>8-by-32-by-16</td>
<td>32x32x16, 32x64x16, 64x32x16</td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>32-by-8-by-16</td>
<td>32x32x16, 32x64x16, 64x32x16</td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>8-by-8-by-32</td>
<td>32x32x32, 32x64x32, 64x32x32, 64x64x32</td>
</tr>
<tr>
<td><strong>WmmaTensorOp</strong></td>
<td>8-by-8-by-128</td>
<td>32x32x128, 32x64x128, 64x32x128, 64x64x128</td>
</tr>
</tbody>
</table><p>CUDA exposes warp-level matrix operations in the CUDA C++ WMMA API. The CUDA C++ WMMA API exposes Tensor Cores via a set of functions and types in the <code class="docutils literal notranslate"><span class="pre">nvcuda::wmma</span></code> namespace. The functions and types in <code class="docutils literal notranslate"><span class="pre">nvcuda::wmma</span></code> provide target-independent APIs and implement architecture-specific tensor operation using TensorOp instruction underneath. CUTLASS exposes WMMA API through WmmaTensorOp. The WmmaTensorOp supports canonical shared memory layouts. The following table summarizes the destination shared memory layout that can be targeted by matrix operands. The WMMA API expects that matrices in shared memory loaded by <code class="docutils literal notranslate"><span class="pre">nvcuda::wmma::load_matrix_sync()</span></code> satisfy 128 bit alignment.</p>
<p><strong>WmmaTensorOp (all matrix sizes and data types).</strong></p>
<table border="1" class="docutils">
<thead>
<tr>
<th><strong>Operand</strong></th>
<th><strong>GMEM Layout</strong></th>
<th><strong>SMEM Layout</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><code>RowMajor</code>, <code>ColumnMajor</code></td>
<td><code>RowMajor</code>, <code>ColumnMajor</code></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><code>RowMajor</code>, <code>ColumnMajor</code></td>
<td><code>RowMajor</code>, <code>ColumnMajor</code></td>
</tr>
<tr>
<td><strong>C</strong></td>
<td><code>RowMajor</code>, <code>ColumnMajor</code></td>
<td><code>RowMajor</code>, <code>ColumnMajor</code></td>
</tr>
</tbody>
</table></section>
</section>
<section id="copyright">
<h1>Copyright<a class="headerlink" href="#copyright" title="Permalink to this heading">¶</a></h1>
<p>Copyright (c) 2017 - 2022 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">Redistribution</span> <span class="ow">and</span> <span class="n">use</span> <span class="ow">in</span> <span class="n">source</span> <span class="ow">and</span> <span class="n">binary</span> <span class="n">forms</span><span class="p">,</span> <span class="k">with</span> <span class="ow">or</span> <span class="n">without</span>
  <span class="n">modification</span><span class="p">,</span> <span class="n">are</span> <span class="n">permitted</span> <span class="n">provided</span> <span class="n">that</span> <span class="n">the</span> <span class="n">following</span> <span class="n">conditions</span> <span class="n">are</span> <span class="n">met</span><span class="p">:</span>

  <span class="mf">1.</span> <span class="n">Redistributions</span> <span class="n">of</span> <span class="n">source</span> <span class="n">code</span> <span class="n">must</span> <span class="n">retain</span> <span class="n">the</span> <span class="n">above</span> <span class="n">copyright</span> <span class="n">notice</span><span class="p">,</span> <span class="n">this</span>
  <span class="nb">list</span> <span class="n">of</span> <span class="n">conditions</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">following</span> <span class="n">disclaimer</span><span class="o">.</span>

  <span class="mf">2.</span> <span class="n">Redistributions</span> <span class="ow">in</span> <span class="n">binary</span> <span class="n">form</span> <span class="n">must</span> <span class="n">reproduce</span> <span class="n">the</span> <span class="n">above</span> <span class="n">copyright</span> <span class="n">notice</span><span class="p">,</span>
  <span class="n">this</span> <span class="nb">list</span> <span class="n">of</span> <span class="n">conditions</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">following</span> <span class="n">disclaimer</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">documentation</span>
  <span class="ow">and</span><span class="o">/</span><span class="ow">or</span> <span class="n">other</span> <span class="n">materials</span> <span class="n">provided</span> <span class="k">with</span> <span class="n">the</span> <span class="n">distribution</span><span class="o">.</span>

  <span class="mf">3.</span> <span class="n">Neither</span> <span class="n">the</span> <span class="n">name</span> <span class="n">of</span> <span class="n">the</span> <span class="n">copyright</span> <span class="n">holder</span> <span class="n">nor</span> <span class="n">the</span> <span class="n">names</span> <span class="n">of</span> <span class="n">its</span>
  <span class="n">contributors</span> <span class="n">may</span> <span class="n">be</span> <span class="n">used</span> <span class="n">to</span> <span class="n">endorse</span> <span class="ow">or</span> <span class="n">promote</span> <span class="n">products</span> <span class="n">derived</span> <span class="kn">from</span>
  <span class="nn">this</span> <span class="n">software</span> <span class="n">without</span> <span class="n">specific</span> <span class="n">prior</span> <span class="n">written</span> <span class="n">permission</span><span class="o">.</span>

  <span class="n">THIS</span> <span class="n">SOFTWARE</span> <span class="n">IS</span> <span class="n">PROVIDED</span> <span class="n">BY</span> <span class="n">THE</span> <span class="n">COPYRIGHT</span> <span class="n">HOLDERS</span> <span class="n">AND</span> <span class="n">CONTRIBUTORS</span> <span class="s2">&quot;AS IS&quot;</span>
  <span class="n">AND</span> <span class="n">ANY</span> <span class="n">EXPRESS</span> <span class="n">OR</span> <span class="n">IMPLIED</span> <span class="n">WARRANTIES</span><span class="p">,</span> <span class="n">INCLUDING</span><span class="p">,</span> <span class="n">BUT</span> <span class="n">NOT</span> <span class="n">LIMITED</span> <span class="n">TO</span><span class="p">,</span> <span class="n">THE</span>
  <span class="n">IMPLIED</span> <span class="n">WARRANTIES</span> <span class="n">OF</span> <span class="n">MERCHANTABILITY</span> <span class="n">AND</span> <span class="n">FITNESS</span> <span class="n">FOR</span> <span class="n">A</span> <span class="n">PARTICULAR</span> <span class="n">PURPOSE</span> <span class="n">ARE</span>
  <span class="n">DISCLAIMED</span><span class="o">.</span> <span class="n">IN</span> <span class="n">NO</span> <span class="n">EVENT</span> <span class="n">SHALL</span> <span class="n">THE</span> <span class="n">COPYRIGHT</span> <span class="n">HOLDER</span> <span class="n">OR</span> <span class="n">CONTRIBUTORS</span> <span class="n">BE</span> <span class="n">LIABLE</span>
  <span class="n">FOR</span> <span class="n">ANY</span> <span class="n">DIRECT</span><span class="p">,</span> <span class="n">INDIRECT</span><span class="p">,</span> <span class="n">INCIDENTAL</span><span class="p">,</span> <span class="n">SPECIAL</span><span class="p">,</span> <span class="n">EXEMPLARY</span><span class="p">,</span> <span class="n">OR</span> <span class="n">CONSEQUENTIAL</span>
  <span class="n">DAMAGES</span> <span class="p">(</span><span class="n">INCLUDING</span><span class="p">,</span> <span class="n">BUT</span> <span class="n">NOT</span> <span class="n">LIMITED</span> <span class="n">TO</span><span class="p">,</span> <span class="n">PROCUREMENT</span> <span class="n">OF</span> <span class="n">SUBSTITUTE</span> <span class="n">GOODS</span> <span class="n">OR</span>
  <span class="n">SERVICES</span><span class="p">;</span> <span class="n">LOSS</span> <span class="n">OF</span> <span class="n">USE</span><span class="p">,</span> <span class="n">DATA</span><span class="p">,</span> <span class="n">OR</span> <span class="n">PROFITS</span><span class="p">;</span> <span class="n">OR</span> <span class="n">BUSINESS</span> <span class="n">INTERRUPTION</span><span class="p">)</span> <span class="n">HOWEVER</span>
  <span class="n">CAUSED</span> <span class="n">AND</span> <span class="n">ON</span> <span class="n">ANY</span> <span class="n">THEORY</span> <span class="n">OF</span> <span class="n">LIABILITY</span><span class="p">,</span> <span class="n">WHETHER</span> <span class="n">IN</span> <span class="n">CONTRACT</span><span class="p">,</span> <span class="n">STRICT</span> <span class="n">LIABILITY</span><span class="p">,</span>
  <span class="n">OR</span> <span class="n">TORT</span> <span class="p">(</span><span class="n">INCLUDING</span> <span class="n">NEGLIGENCE</span> <span class="n">OR</span> <span class="n">OTHERWISE</span><span class="p">)</span> <span class="n">ARISING</span> <span class="n">IN</span> <span class="n">ANY</span> <span class="n">WAY</span> <span class="n">OUT</span> <span class="n">OF</span> <span class="n">THE</span> <span class="n">USE</span>
  <span class="n">OF</span> <span class="n">THIS</span> <span class="n">SOFTWARE</span><span class="p">,</span> <span class="n">EVEN</span> <span class="n">IF</span> <span class="n">ADVISED</span> <span class="n">OF</span> <span class="n">THE</span> <span class="n">POSSIBILITY</span> <span class="n">OF</span> <span class="n">SUCH</span> <span class="n">DAMAGE</span><span class="o">.</span>
</pre></div>
</div>
</section>


                        
                    </div>
                </div>
            </div>
        </div>
    </div>    


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../../../../../../',
            VERSION:'1.0.0',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
    <script type="text/javascript" src="../../../../../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script type="text/javascript" src="../../../../../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../../../../../_static/sphinx_highlight.js"></script>
    <script type="text/javascript" src="../../../../../../../../_static/clipboard.min.js"></script>
    <script type="text/javascript" src="../../../../../../../../_static/copybutton.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script type="text/javascript" src="../../../../../../../../_static/js/theme.js"></script>
  
    <div class="footer" role="contentinfo">
        <div class="container">
            &#169; Copyright bladedisc-dev@list.alibaba-inc.com.
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.3.0.
        </div>
    </div>  

</body>
</html>