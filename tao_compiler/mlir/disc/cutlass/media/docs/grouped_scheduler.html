

<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>CUTLASS Grouped Kernel Schedulers &mdash; BladeDISC 1.0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../../_static/css/theme.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/css/theme.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/copybutton.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 

</head>

<body>
    <header>
        <div class="container">
            <a class="site-nav-toggle hidden-lg-up"><i class="icon-menu"></i></a>
            <a class="site-title" href="../../../../../../index.html">
                BladeDISC
            </a>
        </div>
    </header>


<div class="breadcrumbs-outer hidden-xs-down">
    <div class="container">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="breadcrumbs">
    
      <li><a href="../../../../../../index.html">Docs</a></li>
        
      <li>CUTLASS Grouped Kernel Schedulers</li>
    
    
      <li class="breadcrumbs-aside">
        
            
            <a href="../../../../../../_sources/tao_compiler/mlir/disc/cutlass/media/docs/grouped_scheduler.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>
</div>
    </div>
</div>
    <div class="main-outer">
        <div class="container">
            <div class="row">
                <div class="col-12 col-lg-3 site-nav">
                    
<div role="search">
    <form class="search" action="../../../../../../search.html" method="get">
        <div class="icon-input">
            <input type="text" name="q" placeholder="Search" />
            <span class="icon-search"></span>
        </div>
        <input type="submit" value="Go" class="d-hidden" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
    </form>
</div>
                    <div class="site-nav-tree">
                        
                            
                            
                                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../README.html">BladeDISC Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#what-s-new">What’s New</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#api-quickview">API QuickView</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#setup-and-examples">Setup and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#publications">Publications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#tutorials-and-documents-for-developers">Tutorials and Documents for Developers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#presentations-and-talks">Presentations and Talks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#how-to-contribute">How to Contribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#building-status">Building Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#faq">FAQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../README.html#contact-us">Contact Us</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/install_with_docker.html">Install with Docker</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/install_with_docker.html#download-a-bladedisc-docker-image">Download a BladeDISC Docker Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/install_with_docker.html#start-a-docker-container">Start a Docker Container</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/build_from_source.html">Build from Source</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/build_from_source.html#prerequisite">Prerequisite</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/build_from_source.html#checkout-the-source">Checkout the Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/build_from_source.html#launch-a-development-docker-container">Launch a development Docker container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/build_from_source.html#building-bladedisc-for-tensorflow-users">Building BladeDISC for TensorFlow Users</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/build_from_source.html#building-bladedisc-for-pytorch-users">Building BladeDISC for PyTorch Users</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/quickstart.html#quickstart-for-tensorflow-users">Quickstart for TensorFlow Users</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/quickstart.html#quickstart-for-pytorch-users">Quickstart for PyTorch Users</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/contribution.html">How to Contribute</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/contribution.html#local-development-environment">Local Development Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/contribution.html#submit-a-pull-request-to-bladedisc">Submit a Pull Request to BladeDISC</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/tutorials/index.html">Tutorials on Example Use Cases</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/tutorials/tensorflow_inference_and_training.html">Use case of TensorFlow Inference and Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../docs/tutorials/torch_bert_inference.html">Use case of PyTorch Inference</a></li>
</ul>
</li>
</ul>

                            
                        
                    </div>
                </div>
                <div class="col-12 col-lg-9">
                    <div class="document">
                        
                            
  <p><img alt="ALT" src="media/images/gemm-hierarchy-with-epilogue-no-labels.png" /></p>
<p><a class="reference external" href="/README.md#documentation">README</a> &gt; <strong>Grouped Kernel Schedulers</strong></p>
<section id="cutlass-grouped-kernel-schedulers">
<h1>CUTLASS Grouped Kernel Schedulers<a class="headerlink" href="#cutlass-grouped-kernel-schedulers" title="Permalink to this heading">¶</a></h1>
<p>CUTLASS’s grouped kernel is a persistent kernel which launches multiple problems (e.g., GEMMs, SYR2Ks) within a
single CUDA kernel launch.</p>
<p>Unlike a conventional GEMMs in CUTLASS, which launch a number of threadblocks equal to the number
of tiles in the GEMM, CUTLASS grouped kernels typically launch a number of threadblocks that is
fewer than the total number of tiles across all problems in the group. Each threadblock is then
responsible for computing one or more tiles among the problems in the group. The grouped kernel
<em>scheduler</em> (referred to as the <em>problem visitor</em> in code) is responsible for assigning each
threadblock the sequence of tiles that it will compute within the group.</p>
<p>This document provides background on the functionality of the grouped kernel scheduler, and describes
various optimizations to the grouped kernel scheduler.</p>
<p><strong>Outline</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="#introduction-to-grouped-kernel-schedulers"><span class="std std-ref">Introduction to Grouped Kernel Schedulers</span></a></p></li>
<li><p><a class="reference internal" href="#grouped-gemm-scheduler"><span class="std std-ref">Grouped GEMM Scheduler</span></a></p></li>
<li><p><a class="reference internal" href="#grouped-rank2k-scheduler"><span class="std std-ref">Grouped Rank2K Scheduler</span></a></p></li>
<li><p><a class="reference internal" href="#scheduler-modes"><span class="std std-ref">Scheduler Modes</span></a></p></li>
<li><p><a class="reference internal" href="#improving-load-balance-by-sorting-problems"><span class="std std-ref">Improving Load Balance by Sorting Problems</span></a></p></li>
</ul>
</section>
<section id="introduction-to-grouped-kernel-schedulers">
<h1>Introduction to Grouped Kernel Schedulers<a class="headerlink" href="#introduction-to-grouped-kernel-schedulers" title="Permalink to this heading">¶</a></h1>
<p>Given a group of problem sizes and a grid of threadblocks, the scheduler’s job is to assign
tiles from problems in the group to threadblocks. Threadblocks in a grouped kernel persistently
execute a loop of querying the scheduler for the next tile to compute and performing the
kernel-level operations for that tile (e.g., MMA and epilogue). In pseudocode, this looks as
follows:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">ProblemVisitor</span><span class="w"> </span><span class="n">problem_visitor</span><span class="p">;</span><span class="w"></span>
<span class="w"> </span>
<span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">problem_visitor</span><span class="p">.</span><span class="n">next_tile</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">//</span>
<span class="w">    </span><span class="c1">// Get next tile index from scheduler</span>
<span class="w">    </span><span class="c1">//</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1">//</span>
<span class="w">    </span><span class="c1">// Compute MMA and epilogue</span>
<span class="w">    </span><span class="c1">//</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1">// Inform the scheduler that we are done with the current tile</span>
<span class="w">    </span><span class="n">problem_visitor</span><span class="p">.</span><span class="n">advance</span><span class="p">(</span><span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>The key functionality of the grouped kernel scheduler lies in the <code class="docutils literal notranslate"><span class="pre">next_tile()</span></code> method,
which determines which tile in the group the calling threadblock should compute next, if any.</p>
</section>
<section id="grouped-gemm-scheduler">
<h1>Grouped GEMM Scheduler<a class="headerlink" href="#grouped-gemm-scheduler" title="Permalink to this heading">¶</a></h1>
<p>The scheduler used by grouped GEMM assigns tiles in the group to threadblocks in a round-robin
fashion.</p>
<p>Consider, for example, the threadblock-to-tile mapping that occurs for a group of four GEMMs
each consisting of a grid of 2x2 tiles. Suppose that eight threadblocks are launched. The
figure below illustrates the threadblock ID assigned to each tile in each GEMM in the group.</p>
<p><img alt="ALT" src="media/images/grouped-gemm-schedule-2x2.png" /></p>
<p>A similar mapping for problems that do not have the same number of tiles
is shown below:</p>
<p><img alt="ALT" src="media/images/grouped-gemm-schedule-varied.png" /></p>
<section id="computing-the-schedule-for-a-given-block">
<h2>Computing the schedule for a given block<a class="headerlink" href="#computing-the-schedule-for-a-given-block" title="Permalink to this heading">¶</a></h2>
<p>Each threadblock in the grouped GEMM computes its own schedule by calling
the <code class="docutils literal notranslate"><span class="pre">next_tile()</span></code> method described above.</p>
<p>To do this, the threadblock’s <code class="docutils literal notranslate"><span class="pre">ProblemVisitor</span></code> maintains a <code class="docutils literal notranslate"><span class="pre">thread_idx</span></code>
member that is initialized to <code class="docutils literal notranslate"><span class="pre">blockIdx.x</span></code> and is incremented by
<code class="docutils literal notranslate"><span class="pre">gridDim.x</span></code> between each tile computed (only the x dimension is used)
in the launch configuration for grouped kernels). The scheduler must
then figure out which GEMM in the group <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code> belongs to, and which tile
within that problem it maps to.</p>
<ol class="simple">
<li><p><strong>Determining which GEMM <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code> maps to:</strong> The scheduler determines
the GEMM to which <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code> belongs by iterating through GEMMs starting with
the most-recently visited GEMM, and adding the number of tiles within that
GEMM to a running variable <code class="docutils literal notranslate"><span class="pre">problem_tile_start</span></code>. The scheduler has found the
correct problem for this tile when <code class="docutils literal notranslate"><span class="pre">problem_tile_start</span> <span class="pre">&lt;=</span> <span class="pre">tile_idx</span> <span class="pre">&lt;</span> <span class="pre">problem_tile_start</span> <span class="pre">+</span> <span class="pre">tiles_in_problem</span></code>.</p></li>
<li><p><strong>Determining the tile within a GEMM <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code> maps to:</strong> Once the GEMM
to which <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code> maps has been located, the specific tile within that
GEMM that this block should compute is given by <code class="docutils literal notranslate"><span class="pre">tile_idx</span> <span class="pre">-</span> <span class="pre">problem_tile_start</span></code>.
Simple rasterization is then performed to map this one-dimensional tile ID
into the two-dimensional coordinate of the tile to compute in the GEMM.</p></li>
</ol>
<p>We describe how this search is accelerated in <a class="reference internal" href="#scheduler-modes"><span class="std std-ref">Scheduler Modes</span></a>.</p>
</section>
</section>
<section id="grouped-rank2k-scheduler">
<h1>Grouped Rank2K Scheduler<a class="headerlink" href="#grouped-rank2k-scheduler" title="Permalink to this heading">¶</a></h1>
<p>The previous section described the operation of the scheduler used
for grouped GEMM kernels. While this scheduler is sufficient for
correctly implementing grouped Rank2K operations (i.e., SYR2K and HER2K), it leads to significant inefficiencies.</p>
<p>We next describe these inefficiencies as well as how the CUTLASS
grouped Rank2K scheduler overcomes them.</p>
<section id="inefficiency-of-grouped-gemm-scheduler-for-grouped-rank2k-problems">
<h2>Inefficiency of grouped GEMM scheduler for grouped Rank2K problems<a class="headerlink" href="#inefficiency-of-grouped-gemm-scheduler-for-grouped-rank2k-problems" title="Permalink to this heading">¶</a></h2>
<p>The grouped GEMM scheduler assumes that every tile in every GEMM in the group will
ultimately affect the output of the problem. This is not the case for Rank2K
problems, for which matrix C is either upper or lower triangular. Using the default
grouped GEMM scheduler for such problems will thus lead to threadblocks frequently
being assigned to tiles that exit early (e.g., due to being assigned to a tile in the
upper-triangular portion of a lower-triangular problem). This further leads to load
imbalance among threadblocks, as the grouped GEMM scheduler assigns nearly the same
number of tiles to all threadblocks, regardless of how many tiles are truly active.</p>
<p>Consider an example of a group of four SYR2K problems, each with matrix C consisting
of a grid of 2x2 tiles.  Matrix C in each problem is lower triangular, indicated by
shaded tiles. Consider that eight threadblocks are launched to compute the grouped
problem. The default grouped GEMM scheduler will assign threadblocks to tiles in the following order:</p>
<p><img alt="ALT" src="media/images/grouped-syr2k-schedule-using-grouped-gemm-scheduler.png" /></p>
<p>In this case, threadblocks 1 and 5 are continuously assigned to inactive tiles. In
scenarios in which problems within the group have varying size, we have observed
this to still lead to significant load imbalance.</p>
</section>
<section id="specializing-the-scheduler-for-triangular-problems">
<h2>Specializing the scheduler for triangular problems<a class="headerlink" href="#specializing-the-scheduler-for-triangular-problems" title="Permalink to this heading">¶</a></h2>
<p>We seek to design a scheduler that more efficiently maps threadblocks to active tiles
for kernels that use triangular output matrices. The scheduler should ideally assign
threadblocks only to those tiles within lower-triangular portion of a
lower-triangular problem (and vice-versa for upper-triangular problems).</p>
<p>Using the example above, the resulting assignment of threadblocks to tiles from
such a scheduler might be:</p>
<p><img alt="ALT" src="media/images/grouped-syr2k-schedule-ideal.png" /></p>
<p>Achieving this schedule requires mapping from a threadblock ID to tile coordinates
<code class="docutils literal notranslate"><span class="pre">(i,</span> <span class="pre">j)</span></code>.</p>
<p>We will illustrate this by mapping a lower-triangular matrix with a 3x3 grid. We
first calculate row and column indices assuming one-indexed rows, tiles, and
threadblock IDs, and then subtract one to convert to zero-indexed versions. Our
description borrows heavily from the mapping described <a class="reference external" href="https://stackoverflow.com/a/40954159">here</a>.</p>
<p><img alt="ALT" src="media/images/grouped-syr2k-schedule-3x3.png" /></p>
<section id="calculating-row-i-given-threadblock-id-t">
<h3>Calculating row <code class="docutils literal notranslate"><span class="pre">i</span></code> given threadblock ID <code class="docutils literal notranslate"><span class="pre">t</span></code><a class="headerlink" href="#calculating-row-i-given-threadblock-id-t" title="Permalink to this heading">¶</a></h3>
<p>For a given row i, all threadblock IDs t in that row satisfy the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span> <span class="o">...</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span>
</pre></div>
</div>
<p>The closed-form equation for the right-hand side is: <code class="docutils literal notranslate"><span class="pre">i(i+1)/2</span></code>.
Using this, we can solve for <code class="docutils literal notranslate"><span class="pre">i</span></code> given <code class="docutils literal notranslate"><span class="pre">t</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t</span>  <span class="o">&lt;=</span> <span class="n">i</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
<span class="mi">2</span><span class="n">t</span> <span class="o">&lt;=</span> <span class="n">i</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">i</span>
<span class="mi">2</span><span class="n">t</span> <span class="o">&lt;=</span> <span class="n">i</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.25</span> <span class="o">-</span> <span class="mf">0.25</span>
<span class="mi">2</span><span class="n">t</span> <span class="o">+</span> <span class="mf">0.25</span> <span class="o">&lt;=</span> <span class="n">i</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.25</span>
<span class="mi">2</span><span class="n">t</span> <span class="o">+</span> <span class="mf">0.25</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="n">t</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">&lt;=</span> <span class="n">i</span>
</pre></div>
</div>
<p>To account for fractional values, we set:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="n">t</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<p>To turn this into a zero-indexed row and work with zero-indexed <code class="docutils literal notranslate"><span class="pre">t</span></code>, we perform:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="n">t</span> <span class="o">+</span> <span class="mf">2.25</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</section>
<section id="calculating-column-j-given-threadblock-id-t-and-row-i">
<h3>Calculating column <code class="docutils literal notranslate"><span class="pre">j</span></code> given threadblock ID <code class="docutils literal notranslate"><span class="pre">t</span></code> and row <code class="docutils literal notranslate"><span class="pre">i</span></code><a class="headerlink" href="#calculating-column-j-given-threadblock-id-t-and-row-i" title="Permalink to this heading">¶</a></h3>
<p>For a given row <code class="docutils literal notranslate"><span class="pre">i</span></code>, all threadblock IDs <code class="docutils literal notranslate"><span class="pre">t</span></code> in that row also satisfy the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span> <span class="o">...</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">--&gt;</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="n">i</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</pre></div>
</div>
<p>Threadblock IDs within a given row are sequential, so the one-indexed column ID
for one-indexed threadblock ID <code class="docutils literal notranslate"><span class="pre">t</span></code> and row <code class="docutils literal notranslate"><span class="pre">i</span></code> is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">j</span> <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="p">(</span><span class="n">i</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>The zero-indexed version becomes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">j</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">i</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span><span class="mi">1</span>
  <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="p">(</span><span class="n">i</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="accounting-for-non-square-grids">
<h3>Accounting for non-square grids<a class="headerlink" href="#accounting-for-non-square-grids" title="Permalink to this heading">¶</a></h3>
<p>Though the overall output problem size for Rank2K problems is guaranteed to be square, the
grids used in computing may not be square due to using non-square threadblock shapes. For
example, a threadblock shape of 64x32 operating on a problem of output size 128x128 would
result in a grid of 2x4 tiles.</p>
<p>This case can be handled by noting that the output resembles a square grid of 2x2 “macro tiles”
each of which contains 2 “true tiles.” We can thus first map a threadblock ID to its “macro tile”
using the equations above, and then map it to the “true tile” within its “macro tile.” In the example
of a 2x4 grid, this mapping would look as follows:</p>
<p><img alt="ALT" src="media/images/grouped-syr2k-schedule-macro.png" /></p>
<p>A zero-indexed threadblock ID <code class="docutils literal notranslate"><span class="pre">t</span></code> is mapped to its “macro tile ID” <code class="docutils literal notranslate"><span class="pre">t_macro</span></code> as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t_macro</span> <span class="o">=</span> <span class="n">t</span> <span class="o">//</span> <span class="n">r</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">r</span></code> is the ratio of the maximum dimension of the grid to the
minimum dimension of the grid (i.e., <code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">4</span> <span class="pre">/</span> <span class="pre">2</span> <span class="pre">=</span> <span class="pre">2</span></code> in the previous example).</p>
<p>One uses <code class="docutils literal notranslate"><span class="pre">t_macro</span></code> and the calculations above to find the row and column in the square matrix to
obtain <code class="docutils literal notranslate"><span class="pre">i_macro</span></code> and <code class="docutils literal notranslate"><span class="pre">j_macro</span></code> (zero-indexed). The mapping from <code class="docutils literal notranslate"><span class="pre">(i_macro,</span> <span class="pre">j_macro)</span> <span class="pre">--&gt;</span> <span class="pre">(i,</span> <span class="pre">j)</span></code>
is simply the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="p">(</span><span class="n">ThreadblockShape</span><span class="p">::</span><span class="n">M</span> <span class="o">&gt;</span> <span class="n">ThreadblockShape</span><span class="p">::</span><span class="n">N</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">ThreadblockShape</span><span class="p">::</span><span class="n">M</span> <span class="o">/</span> <span class="n">ThreadblockShape</span><span class="p">::</span><span class="n">N</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i_macro</span>
    <span class="n">j</span> <span class="o">=</span> <span class="p">(</span><span class="n">j_macro</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">t</span> <span class="o">%</span> <span class="n">r</span><span class="p">)</span>
<span class="k">elif</span> <span class="p">(</span><span class="n">ThreadblockShape</span><span class="p">::</span><span class="n">M</span> <span class="o">&lt;</span> <span class="n">ThreadblockShape</span><span class="p">::</span><span class="n">N</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">ThreadblockShape</span><span class="p">::</span><span class="n">N</span> <span class="o">/</span> <span class="n">ThreadblockShape</span><span class="p">::</span><span class="n">M</span>
    <span class="n">i</span> <span class="o">=</span> <span class="p">(</span><span class="n">i_macro</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">t</span> <span class="o">%</span> <span class="n">r</span><span class="p">)</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">j_macro</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i_macro</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">j_macro</span>
</pre></div>
</div>
</section>
<section id="handling-cases-with-grid-dimensions-that-aren-t-multiples-of-each-other">
<h3>Handling cases with grid dimensions that aren’t multiples of each other<a class="headerlink" href="#handling-cases-with-grid-dimensions-that-aren-t-multiples-of-each-other" title="Permalink to this heading">¶</a></h3>
<p>Even though threadblock shapes M and N are typically multiples of one another, the grid
for a given problem may not have dimensions of the same ratio as that of the threadblock.
For example, a problem of size 132x132 using a threadblock of shape 64x32 will result
in a grid of 3x5 tiles. In this case, there is not an integer number of “true tiles”
per “macro tile.”</p>
<p>When this scenario arises, we simply pad the larger dimension of the grid such that
there are an integer number of “true tiles” per “macro tile.” Thus, the 3x5 grid in
the example above will be treated as a 3x6 grid. Row and column positions for each
tile are calculated as above. Any threadblocks that map to tiles that are outside the
problem range or upper/lower triangular portion (e.g., (2, 5)) will exit early from
this problem and may proceed to the next problem in the group.</p>
</section>
<section id="handling-upper-triangular-matrices">
<h3>Handling upper-triangular matrices<a class="headerlink" href="#handling-upper-triangular-matrices" title="Permalink to this heading">¶</a></h3>
<p>The only modification needed for upper-triangular matrices is to swap <code class="docutils literal notranslate"><span class="pre">i_macro</span></code> and <code class="docutils literal notranslate"><span class="pre">j_macro</span></code> in the calculations above.</p>
</section>
</section>
</section>
<section id="scheduler-modes">
<h1>Scheduler modes<a class="headerlink" href="#scheduler-modes" title="Permalink to this heading">¶</a></h1>
<p>The grouped kernel schedulers come with two different modes for finding
the next tile for a block to compute. These techniques are controlled by
the <a class="reference external" href="../../include/cutlass/gemm/kernel/grouped_problem_visitor.h"><code class="docutils literal notranslate"><span class="pre">cutlass::gemm::kernel::GroupScheduleMode</span></code></a> enum.
We describe each mode in greater detail below.</p>
<section id="groupschedulemode-kdeviceonly-default">
<h2><code class="docutils literal notranslate"><span class="pre">GroupScheduleMode::kDeviceOnly</span></code> (default)<a class="headerlink" href="#groupschedulemode-kdeviceonly-default" title="Permalink to this heading">¶</a></h2>
<p>This scheduler mode performs all scheduling work on the device. It parallelizes
the search for the problem that <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code> maps to by having each thread “own”
a different problem and determine whether <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code> falls within the range of
that problem.</p>
<p><code class="docutils literal notranslate"><span class="pre">GroupScheduleMode::kDeviceOnly</span></code> performs this parallelization in a warp-wide
fashion. Each thread in the warp loads a problem size indexed by its lane id and
computes the number of tiles in that problem. A warp-wide prefix sum is used to find
the starting tiles for the set of problems the warp is looking at. At the end of the
prefix sum, each thread holds the starting tile index and tile count for a unique
problem in the group.</p>
<p>While <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code> remains within the range of the problems currently hosted by the
warp, each thread will check whether <code class="docutils literal notranslate"><span class="pre">tile_idx</span></code>  is in the range of its current
problem. The matching problem index and its starting tile are then broadcasted to all
threads in the warp.</p>
</section>
<section id="precomputing-schedules-on-the-host-groupschedulemode-khostprecompute">
<h2>Precomputing schedules on the host: <code class="docutils literal notranslate"><span class="pre">GroupScheduleMode::kHostPrecompute</span></code><a class="headerlink" href="#precomputing-schedules-on-the-host-groupschedulemode-khostprecompute" title="Permalink to this heading">¶</a></h2>
<p>This scheduler attempts to reduce the amount of scheduling performed on the device
by precomputing on the host the sequence of problems that will
be accessed by each block. As described above, all that is needed to map tile_idx  to
the specific tile within a problem to compute is the problem ID and the problem’s
starting tile (among all of the tiles in the group). Thus, this scheduler precomputes
the problem index and problem starting tile for each tile computed by each block.</p>
<p>The schedule for an individual block is represented as an array of
<code class="docutils literal notranslate"><span class="pre">(problem_idx,</span> <span class="pre">problem_starting_tile)</span></code> tuples. There is one such array per block.
These arrays are produced on the host and copied over to the device. This
representation is optimized for the case in which blocks compute at most one
tile per problem. When a block computes multiple tiles per problem in the group,
the representation above will result in duplicate entries, and thus will be
suboptimal (e.g., <code class="docutils literal notranslate"><span class="pre">[(3,</span> <span class="pre">20),</span> <span class="pre">(3,</span> <span class="pre">20)]</span></code> for a block that computes two tiles in
problem 3, which has starting tile index 20).
We have chosen to use the representation described above because grouped kernels
themselves are typically most beneficial when problem sizes are small, and, thus,
blocks compute at most one tile per problem.</p>
</section>
<section id="which-scheduler-mode-should-i-use">
<h2>Which scheduler mode should I use?<a class="headerlink" href="#which-scheduler-mode-should-i-use" title="Permalink to this heading">¶</a></h2>
<p>Consider the following questions when deciding which scheduling mode to use:</p>
<section id="how-are-the-parameters-used-as-input-to-the-grouped-kernel-e-g-ptra-lda-set-in-my-application">
<h3>How are the parameters used as input to the grouped kernel (e.g., ptrA, lda) set in my application?<a class="headerlink" href="#how-are-the-parameters-used-as-input-to-the-grouped-kernel-e-g-ptra-lda-set-in-my-application" title="Permalink to this heading">¶</a></h3>
<p>If these are set by a previous kernel running on
the device (rather than by the host), you likely want to use <code class="docutils literal notranslate"><span class="pre">kDeviceOnly</span></code>,
as this will minimize additional host-device communication.</p>
</section>
<section id="can-host-side-work-be-overlapped-with-other-device-kernels-in-my-application">
<h3>Can host-side work be overlapped with other device kernels in my application?<a class="headerlink" href="#can-host-side-work-be-overlapped-with-other-device-kernels-in-my-application" title="Permalink to this heading">¶</a></h3>
<p>For example, if a grouped GEMM is used as the Nth layer in a neural network,
host-side precomputation for the grouped GEMM can potentially be overlapped
with device-side work for layer N-1. In this case <code class="docutils literal notranslate"><span class="pre">kHostPrecompute</span></code> is likely
a good fit.</p>
</section>
<section id="how-compute-intensive-are-the-problems-in-my-group">
<h3>How compute-intensive are the problems in my group?<a class="headerlink" href="#how-compute-intensive-are-the-problems-in-my-group" title="Permalink to this heading">¶</a></h3>
<p>The differences in performance between <code class="docutils literal notranslate"><span class="pre">kHostPrecompute</span></code> and <code class="docutils literal notranslate"><span class="pre">kDeviceOnly</span></code> are most
noticeable for grouped kernels with low computational intensity, for which time spent in
the scheduler accounts for a significant fraction of the grouped kernel’s runtime.
Intuitively, as problems in a group decrease in computational intensity, a smaller
fraction of the overall runtime will be consumed in performing MMA operations, leading
to a larger fraction of the overall runtime being consumed by scheduling logic.</p>
<p>Since the scheduling modes affect only the scheduling logic of the grouped kernels,
one expects to see most benefit from <code class="docutils literal notranslate"><span class="pre">kHostPrecompute</span></code> for less computationally-intense
groups.</p>
</section>
</section>
</section>
<section id="improving-load-balance-by-sorting-problems">
<h1>Improving Load Balance by Sorting Problems<a class="headerlink" href="#improving-load-balance-by-sorting-problems" title="Permalink to this heading">¶</a></h1>
<p>The grouped kernel schedulers assign a nearly equal number
of tiles to each block participating in the grouped kernel. Every tile in the
group has the same M and N dimensions. However, the K dimension of each
tile depends on the K dimension of the problem, so tiles may have different
K dimensions. Thus, the K dimension of the
tile plays a significant role in determining how long it takes for a given
tile to be computed.</p>
<section id="potential-problems-with-imbalanced-k-dimension">
<h2>Potential problems with imbalanced K dimension<a class="headerlink" href="#potential-problems-with-imbalanced-k-dimension" title="Permalink to this heading">¶</a></h2>
<p>To ensure that compute load is balanced evenly across blocks, it is important
that the sum of the K dimensions among all tiles a block computes be similar
to that of other blocks; if one block computes far more tiles with a large
value of K than other blocks, it may take longer than the other blocks.</p>
<p>For example, consider the following group of GEMMs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mi">1152</span><span class="n">x768x128</span>
<span class="mi">1</span> <span class="mi">1152</span><span class="n">x768x1024</span>
<span class="mi">2</span> <span class="mi">768</span><span class="n">x1152x128</span>
<span class="mi">3</span> <span class="mi">768</span><span class="n">x1152x1024</span>
</pre></div>
</div>
<p>If a tile size of 128x128 is used, then each problem will have 54 tiles.
Thus, there are 216 tiles across the group.</p>
<p>Suppose this grouped GEMM is run on GA100, which has 108 SMs. Suppose that
the occupancy given the parameters of the grouped GEMM is one – one threadblock
can be active at a time on an SM. The grouped GEMM will, thus, run with 108
persistent threadblocks, each of which computes (256 / 108) = 2 tiles.</p>
<p>Under the round-robin assignment of tiles to threadblocks employed by
the grouped GEMM scheduler, the assignment of tiles to threadblocks
in this GEMM will be as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Threadblocks</span> <span class="mi">0</span><span class="o">-</span><span class="mi">53</span><span class="p">:</span>     <span class="n">Tiles</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">128</span><span class="n">x128x128</span>  <span class="kn">from</span> <span class="nn">problem</span> <span class="mi">0</span>
<span class="n">Threadblocks</span> <span class="mi">54</span><span class="o">-</span><span class="mi">107</span><span class="p">:</span>   <span class="n">Tiles</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">128</span><span class="n">x128x1024</span> <span class="kn">from</span> <span class="nn">problem</span> <span class="mi">1</span>
<span class="n">Threadblocks</span> <span class="mi">0</span><span class="o">-</span><span class="mi">53</span><span class="p">:</span>     <span class="n">Tiles</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">128</span><span class="n">x128x128</span>  <span class="kn">from</span> <span class="nn">problem</span> <span class="mi">2</span>
<span class="n">Threadblocks</span> <span class="mi">54</span><span class="o">-</span><span class="mi">107</span><span class="p">:</span>   <span class="n">Tiles</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">128</span><span class="n">x128x1024</span> <span class="kn">from</span> <span class="nn">problem</span> <span class="mi">3</span>
</pre></div>
</div>
<p>Following this assignment, threadblocks 54-107 perform significantly more
work than threadblocks 0-53 because they compute two tiles with a K
dimension of 1024, whereas threadblocks 0-53 compute two tiles with K
dimension of only 128.</p>
<p>Due to this imbalanced assignment, threadblocks 54-107 will run
significantly longer than threadblocks 0-53, leaving threadblocks
0-53 idle for a large fraction of time.</p>
<p>Clearly, a better assignment of tiles to threadblocks for this
example would involve all threadblocks computing one tile with
a K dimension of 1024 and one tile with a K dimension of 128.
This would better balance the workload among threadblocks.</p>
</section>
<section id="potential-for-sorting-problems-to-reduce-imbalance">
<h2>Potential for sorting problems to reduce imbalance<a class="headerlink" href="#potential-for-sorting-problems-to-reduce-imbalance" title="Permalink to this heading">¶</a></h2>
<p>A simple way to potentially reduce load imbalance is to sort the problems in a group in
descending order of their K dimension. This can help to improve load balance
because tiles in a group are assigned in a round-robin fashion to blocks
sequentially, so every block will always be assigned next the tile with
the highest K dimension available.</p>
<p>Considering the example described above, sorting the problem sizes before
executing grouped GEMM improves the runtime of this grouped GEMM on GA100 with each
scheduling mode by around 30%.</p>
<p>To ease the process of sorting groups and their associated metadata in this
manner, the device-level grouped kernels provide a <code class="docutils literal notranslate"><span class="pre">sort_problems()</span></code> method.
An example of how to use this may be found in the <a class="reference external" href="../../examples/24_gemm_grouped/gemm_grouped.cu">grouped GEMM example</a>.</p>
<p>Finally, while sorting problems can be helpful in certain scenarios, it is
not guaranteed to improve performance. In some cases, performance can
decrease when sorting problems due to additional conflicting factors that
affect GEMM performance. We recommend profiling your grouped kernel with
and without sorting to see whether it helps in your case.</p>
</section>
</section>


                        
                    </div>
                </div>
            </div>
        </div>
    </div>    


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../../../../',
            VERSION:'1.0.0',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
    <script type="text/javascript" src="../../../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script type="text/javascript" src="../../../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../../../_static/sphinx_highlight.js"></script>
    <script type="text/javascript" src="../../../../../../_static/clipboard.min.js"></script>
    <script type="text/javascript" src="../../../../../../_static/copybutton.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script type="text/javascript" src="../../../../../../_static/js/theme.js"></script>
  
    <div class="footer" role="contentinfo">
        <div class="container">
            &#169; Copyright bladedisc-dev@list.alibaba-inc.com.
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.3.0.
        </div>
    </div>  

</body>
</html>